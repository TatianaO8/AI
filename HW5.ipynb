{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOP4wSnk25CGM6qLJ/b89v8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TatianaO8/AI/blob/master/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGDBgbq3vC5i",
        "colab_type": "text"
      },
      "source": [
        "# Homework 5\n",
        "\n",
        "Summarize and describe the different concepts/methods/algorithms that you have learned in this course.\n",
        "\n",
        "Use a Colab notebook. Make sure that you organize the material logically by using sections/subsections. Also, use code cell to include code snippets.\n",
        "\n",
        "I suggest that you group everything into five categories:\n",
        "\n",
        "1. General concepts (for instance, what is artificial intelligence, machine learning, deep learning)\n",
        "\n",
        "2. Basic concepts (for instance, here you can talk about linear regression, logistic regression, gradients, gradient descent)\n",
        "\n",
        "3. Building a model (for instance, here you can talk about the structure of a convent, what it components are etc.)\n",
        "\n",
        "4. Comping a model (for instance, you can talk here about optimizers, learning rate etc.)\n",
        "\n",
        "5. Training a model (for instance, you can talk about overfitting/underfitting)\n",
        "\n",
        "6. Finetuning a pretrained model (describe how you proceed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYOPvY-BvVTU",
        "colab_type": "text"
      },
      "source": [
        "## 1. General Concepts:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erNywgbTvbjt",
        "colab_type": "text"
      },
      "source": [
        "Deep learning is a subset of machine learning. Machine learning is a subset of artificial intelligence (AI).\n",
        "\n",
        "Artificial intelligence is a branch of computer science that converts human intelligence into a computer.\n",
        "\n",
        "Machine learning is the process of the computer learning by examples/data it is exposed to. ML relies less on human made code compared to general AI because it learns more and more as more data is inputted; unlike AI which is a code of rules it follows. For every certain type of input, there is an output. \n",
        "\n",
        "ML is the process of training a model to make useful predictions using a data set. Then, use this prediction model to predict unseen data output. \n",
        "\n",
        "Deep learning has networks capable of learning unsupervised from data that is unstructured and unlabeled. \n",
        "\n",
        "Examples Of AI: Speech recognition, decision-making, translation, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3MV2TZtylsE",
        "colab_type": "text"
      },
      "source": [
        "### Machine Learning\n",
        "\n",
        "Three types of ML: \n",
        "\n",
        "*   Supervised learning - the model is provided with labeled training data \n",
        "> *   Predict label based off features \n",
        "> > *   Features are the things you describe the label you want the model to predict off the features given\n",
        "> *   In supervised machine learning, you feed the features and their corresponding labels into an algorithm in a process called training \n",
        "> > *   During training, the algorithm gradually determines the relationship between features and their corresponding labels. This relationship is called the model.  \n",
        "> *   To tie it all together, supervised machine learning finds patterns between data and labels that can be expressed mathematically as functions \n",
        "> *   Given an input feature, you are telling the system what the expected output label is, thus you are supervising the training. The ML system will learn patterns on this labeled data.  \n",
        "> *   Predict label based off features data and must infer its own rules for doing so \n",
        "\n",
        "\n",
        "*   Unsupervised learning – the model has no hints how to categorize each piece of \n",
        "> * Identify meaningful patterns in the data \n",
        "> * The machine must learn from an unlabeled data set\n",
        "\n",
        "* Reinforcement learning – you don't collect examples with labels \n",
        "> * You set up a model (often called an agent in RL) with the game, and you tell the model not to get a \"game over\" screen \n",
        "> * Receives reward r from each action it takes at each state and then moves on to the next one "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0BlCEnxLqPh",
        "colab_type": "text"
      },
      "source": [
        "### Deep Learning\n",
        "\n",
        "A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. \n",
        ">* Densely connected layers or fully connected layers - used for simple vector data stored in 2D tensors of shape (samples, features)\n",
        ">* Recurrent layers - used for sequence data stored in 3D tensors of shape (samples, timesteps, features)\n",
        ">>* The most commonly used is long-short term memory (LSTM)\n",
        ">* Convolutional layers - used for image data stored in 4D tensors\n",
        "\n",
        "Deep learning models in Keras are made by using a combination and/or many of these layers to form useful data-processing pipelines.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D4hgwtOOXzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Showing a simple Keras model\n",
        "# It uses a dense layer\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(1, input_shape=(1,)))\n",
        "\n",
        "network.compile(optimizer='sgd', loss='mse', metrics=['mse'])\n",
        "\n",
        "network.fit(train_xs, train_ys, epochs=10, batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxlVYlXQ0ypi",
        "colab_type": "text"
      },
      "source": [
        "## 2. Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RyltWaQ02F0",
        "colab_type": "text"
      },
      "source": [
        "A regression model predicts continuous values. \n",
        "\n",
        "A classification model predicts discrete values.\n",
        "\n",
        "When training a model, the goal, on average, is to have low loss across all examples by findind the right set of weights and biases. (Loss is the penalty for a bad prediction. Thus, a perfect prediction is 0 and anything else is above that.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3X6nE5J15xQ",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n",
        "\n",
        "In linear regression, you could draw a straight line to show a relationship between variables. It might not necessairly pass through every dot, but it gives a good idea and shows the relationship clearly. \n",
        "\n",
        "The equation for a line in ML: ŷ = b + Σ$w_jx_j$... where\n",
        "*   ŷ - predicted label\n",
        "*   b - bias or $w_0$\n",
        "*   $w_j$ is the weight of feature j = 1,2,...\n",
        "*   $x_j$ is the jth feature (input)\n",
        "\n",
        "It uses a loss function called mean squared loss.\n",
        "Mean squared loss = (1/m)Σ.5(y-ŷ)$^2$, m = # of inputs\n",
        "\n",
        "The model iteratively adjusts the weight and bias until the loss stops changing or changes by the slightest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK5yIEaBIOkE",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n",
        "To more efficient, there is a better mechanism/algorithm called gradient descent.\n",
        "\n",
        "The gradient ▽L is a vector whose entries are partial derivatives of the loss function.\n",
        "> *  Has both a direction and a magnitude.\n",
        "> * Takes a step in the direction of the negative gradient to reduce the loss\n",
        "> * Gradient descent then updates the gradient:\n",
        "> > w <- w - α▽L       where α = learning rate\n",
        "\n",
        "\n",
        "A batch is the total number of examples you use to calculate the gradient in a single iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLvA4R_LIPuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs\n",
        "epochs = 10\n",
        "# learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# initial value for weight w and bias b\n",
        "w = np.random.randn(1)\n",
        "b = np.zeros(1)\n",
        "\n",
        "for epoch in np.arange(epochs):\n",
        "  for i in np.arange(80):\n",
        "    y_pred = w * train_xs[i] + b\n",
        "    \n",
        "    grad_w = (y_pred - train_ys[i]) * train_xs[i]\n",
        "    grad_b = (y_pred - train_ys[i])\n",
        "    \n",
        "    w -= lr * grad_w\n",
        "    b -= lr * grad_b\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtdG91_JJ97r",
        "colab_type": "text"
      },
      "source": [
        "#### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "SGD uses one example per iteration (batch size is 1) and it is chosen at random.\n",
        "It makes a lot of noise, a lot of randomness in the dataset.\n",
        "\n",
        "#### Mini-batch Stochastic Gradient Descent (mini-batch SGD)\n",
        "\n",
        "This is the middle ground between a full-batch iteration and SGD. In mini-batch SGD, 10 to 10,000 random example are chosen per iteration.\n",
        "The amount of noise is again in between both using full-batch and SGD; reduces the amount of noise in SGD but is still more efficient than full-batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjIN5rV1PflD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Snippet of SGD\n",
        "\n",
        "weight_path_sgd.append(weight)\n",
        "for epoch in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    \n",
        "    for i in range(m):           \n",
        "        xi = X_b_shuffled[i:i+1]\n",
        "        yi = y_shuffled[i:i+1]\n",
        "        gradient = xi.T.dot(xi.dot(weight) - yi)\n",
        "        weight = weight - lr * gradient\n",
        "        weight_path_sgd.append(weight)\n",
        "        \n",
        "        y_predict = X_new_b.dot(weight)                    \n",
        "        plt.plot(X_new, y_predict, \"b-\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUCUwa0jPiIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Snippet of mini-batch SGD\n",
        "\n",
        "weight_path_mgd.append(weight)\n",
        "for epoch in range(epochs):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, batch_size):\n",
        "        xi = X_b_shuffled[i:i+batch_size]\n",
        "        yi = y_shuffled[i:i+batch_size]\n",
        "        gradient = 1 / batch_size * xi.T.dot(xi.dot(weight) - yi)\n",
        "        weight = weight - lr * gradient\n",
        "        weight_path_mgd.append(weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgMUTl9s4aMw",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is used for binary classification problems meaning there are only two classes.\n",
        "*   Uses the sigmoid activation function: guarantees a valid response between 0 and 1\n",
        ">  (1/(1 + e^-z))     where z = Σ$w_jx_j$ + b\n",
        "*   Most of the time uses binary cross entropy loss: Loss (L) = -ylog(a) - (1-y)log(1-a)\n",
        "\n",
        "Thus, is the sigmoid function is anything less than .5, the example is labeled as class 0. On the other hand, if the sigmoid function is anything more than or equal to .5, the example is labeled as class 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJwPPkzJSaaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression in Keras\n",
        "\n",
        "def build_and_compile_model():\n",
        "    # build model\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    layer = tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(2,))\n",
        "    model.add(layer)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(data_training, labels_training, epochs=10, batch_size=512)\n",
        "    weights, bias = layer.get_weights()\n",
        "    \n",
        "    return model, weights, bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7bTA6uTH7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "def sigmoidFunction(z):\n",
        "  return 1/(1 + math.exp(-z))\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  shuffled_indices = np.random.permutation(m)\n",
        "\n",
        "  for i in range(m):\n",
        "    z = X[i].dot(weights) + b\n",
        "    a = sigmoidFunction(z)\n",
        "    gradient_w = (a - y[i]) * X[i]\n",
        "    gradient_b = (a-y[i])   \n",
        "    weights = weights - lr * gradient_w\n",
        "    b = b - lr * gradient_b\n",
        "\n",
        "display_random_data_with_lines(labels, data, weights, b, initial_weight, initial_b)\n",
        "\n",
        "#compute the binary cross entropy and accuracy on the test set\n",
        "binary_cross_entropy_loss = 0\n",
        "correct_count = 0\n",
        "\n",
        "for i in range(len(data_test)):\n",
        "  a = sigmoidFunction(data_test[i].dot(weights) + b)\n",
        "  binary_cross_entropy_loss += -1*labels_test[i]*math.log(a,2)- (1-labels_test[i])*math.log(a,2)\n",
        "\n",
        "  if (a < .5 and labels_test[i] == 0) or (a >= .5 and labels_test[i] == 1):\n",
        "    correct_count += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3sreY2w4xe1",
        "colab_type": "text"
      },
      "source": [
        "## 3. Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxU3dIh7TPtd",
        "colab_type": "text"
      },
      "source": [
        "### Convent\n",
        "\n",
        "Convent is another name for convolutional neural networks (CNN).\n",
        "CNN's are used when building models for image classification because it can extract a higher representation of pixels. It is different from other networks in that it doesn't take in textures or shapes; it just takes in the raw pixels as input and learns how to extract features and learns to predict what the image is. \n",
        "\n",
        "*   Convolution \n",
        ">* extracts tiles of the input feature map and applies a filter to it to compute new features, producing an output feature map\n",
        ">>* filters are matrices \n",
        ">>* during training, the CNN learns the optimal values for the filter matrices to enable it to extract meaningful features from the input feature map\n",
        ">>* as more filters are applied to the input feature map, the more features extracted but at a cost of time increasing\n",
        ">>* each additional filter has less value so the goal is to use the minimum amount of filters with the best interpretation of the input\n",
        ">* the filter slide over the input feature map one pixel at a time\n",
        ">* padding can be used for preservation\n",
        ">>* In the early layers of our network, we want to preserve as much information about the original input volume so that we can extract those low level features \n",
        "*   ReLU - stands for rectified linear activation unit and is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero: max(0, x)\n",
        "*   MaxPooling - An algorithm that reduces the number of dimensions of the feature map while still preserving the most critical feature information\n",
        "> It also uses a feature map and extracts tiles except we take the maximum value of each tile and input it into our new feature map\n",
        ">> tiles are moved by a defined stride, a scalar value\n",
        "\n",
        "\n",
        "How it works:\n",
        "1. The CNN receives an input feature map, a 3D matrix\n",
        "2. Uses convolution, ReLU, and pooling for feature extraction\n",
        "3. One or more fully connected layers\n",
        "4. Last layers is for classification using softmax activation function outputting a probability value from 0 to 1 on each classification label prediction\n",
        "\n",
        "\n",
        "A basic convnet is made up of a stack of Conv2D and MaxPooling2D layers. Then, the output is a 3D tensor. The width and height dimensions tend to shrink as you go deeper in the network. Then the next step to feed the last output tensor into a densely connected classifier network. \n",
        "\n",
        "Convnets have two interesting properties: the patterns they learn are translation invariant and they can learn spatial hierarchies of patterns. This gives it an advantage over a densely connected network. A densely connected network has to relearn patterns if found in a new location unlike convnets that learn in a more general form. Thus, convnets need fewer samples to learn representations that give generalization power. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO935Vv3VMk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolution 2D\n",
        "\n",
        "def conv2d(input_mat, kernel_mat):\n",
        "  s = kernel_mat.shape[0]\n",
        "  m = input_mat.shape[0]\n",
        "\n",
        "  # Check for invalid convolution\n",
        "  if s > m:\n",
        "    raise Exception('Invalid convolution inputs. The kernel matrix size is greater than the input matrix size.')\n",
        "\n",
        "  # Calculate the size of the output matrix\n",
        "  n = m - s + 1\n",
        "\n",
        "  # Create an empty output matrix\n",
        "  output_mat = np.zeros((n,n))\n",
        "  \n",
        "  for i in np.arange(n):\n",
        "    for j in np.arange(n):\n",
        "      for r in np.arange(s):\n",
        "        for p in np.arange(s): \n",
        "          output_mat[i][j] += kernel_mat[r][p] * input_mat[i+r][j+p]\n",
        "  \n",
        "\n",
        "  return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJPmoM3tVO9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maxpooling 2D\n",
        "\n",
        "def maxpooling2d(input_mat, s):\n",
        "  m = input_mat.shape[0]\n",
        "\n",
        "  if s > m:\n",
        "    raise Exception('Invalid maxpooling inputs. s is larger than the input_matrix size.')\n",
        "  \n",
        "  n = int(m/s)\n",
        "  output_mat = np.zeros((n,n))\n",
        "\n",
        "  for i in np.arange(n):\n",
        "    for j in np.arange(n):\n",
        "      output_mat[i][j] = input_mat[i*s][j*s]\n",
        "      for r in np.arange(s):\n",
        "        for p in np.arange(s): \n",
        "          output_mat[i][j] = max(output_mat[i][j], input_mat[r+i*s][p+j*s])\n",
        "  \n",
        "  return output_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JezXRZ_oZnZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Neural Network\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32,(5,5),activation=’relu’,input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (5, 5), activation=’relu’))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10, activation=’softmax’))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovfuRy89IcW4",
        "colab_type": "text"
      },
      "source": [
        "## 4. Comping a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjissKNHYkW",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate\n",
        "\n",
        "The learning rate is symbolized as α and is a scalar value.\n",
        "It is also referred to as the step size. This is used in gradient descent to update the gradient as I talked and showed before. \n",
        "\n",
        "The learning rate is a hyperparamter meaning it is external to the model. It is something we define when we train the model. \n",
        "\n",
        "If learning rate is too small, the model might take a long time to learn. On the other hand, if the learning rate is too big, the model might not ever get to its ideal point because it might skip over it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md6Sp4TwOnlo",
        "colab_type": "text"
      },
      "source": [
        "### Epoch\n",
        "\n",
        "Epoch in machine learning is the number of iterations of the dataset.\n",
        "\n",
        "This is also a hyperparameter and is defined when setting the parameters of your model.\n",
        "\n",
        "(You can see these hyperparamters in some of the code shown in here.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQMuMDxRaoFQ",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "Optimizers determines how the model will update during training.\n",
        "\n",
        "An example is gradient descent, which was exaplained above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef6TFec7bVF4",
        "colab_type": "text"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "A loss function is the quantity that will be minimzed during training.\n",
        "\n",
        "There are a few loss functions, but there is pretty much a set pair between the problem type and the loss function.\n",
        "\n",
        "binary classification -> binary_crossentropy, activation: sigmoid\n",
        "\n",
        "multiclass, single-label classification -> categorical_crossentropy/binary_crossentropy, activation: softmax/sigmoid\n",
        "\n",
        "regression to arbituary values -> mse, activation: none\n",
        "\n",
        "regression to values in [0,1] -> mse or binary_crossentropy, activation: sigmoid\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddnx5mFK1xp",
        "colab_type": "text"
      },
      "source": [
        "## 5. Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPZWYDWUcYW0",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting/Underfitting\n",
        "\n",
        "Overfitting is when a model trains the data too well, so it bad at predicting.\n",
        "*   Model has low loss on training data, high loss on test data\n",
        "* Model is probably more complex than it has to be and didn't generalize well\n",
        "\n",
        "Underfitting is when a model does not fit the data well enough, so it can't capture a trend in the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnef6koBccw7",
        "colab_type": "text"
      },
      "source": [
        "###Training/Test/Validation Sets\n",
        "\n",
        "To train a model, we need to input it data. This is where training, validation, and testing data comes in.\n",
        "\n",
        "To prevent overfitting, we want the model to predict on values it has never seen before. Thus, we split the data into two data sets if the data is large enough, training and testing sets. The training set is the data used to train the model and the testing set is the data used to make predictions with the new model to see how good of a model it is.\n",
        "\n",
        "\n",
        "If the data is not large enough and to reduce overfitting, we use validation sets as well. The validation set is used to test the training set and tune model parameters accoridingly and once it passes that test, it will test on the testing set. \n",
        "\n",
        "There are a few ways to split this data. \n",
        "*   Simple hold-out\n",
        "> Split data into training set, validation set, and testing set\n",
        "> Flaw: if the validation set is too small, it might not represent the data as a whole\n",
        "*   K-fold validation\n",
        "> Used when you have little data\n",
        "> Split your data into K partitions of equal size \n",
        "> For each partition i, train a model on the remaining K - 1 partitions, and evaluate it on partition i\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "training set -> validation set -> testing set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qKoApBcknHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = data[num_validation_samples:]\n",
        "validation_data = data[:num_validation_samples]\n",
        "\n",
        "model = get_model()\n",
        "model.train(training_data)\n",
        "\n",
        "validation_score = model.evaluate(validation_data)\n",
        "model = get_model()\n",
        "model.train(data)\n",
        "\n",
        "test_score = model.evaluate(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9UrOWmJQAO7",
        "colab_type": "text"
      },
      "source": [
        "## Finetuning a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N8fFdhCQIJn",
        "colab_type": "text"
      },
      "source": [
        "To understand what finetuning is we have to understand what a pre-trained model and what ‘freezing’ means. \n",
        "\n",
        "A pretrained model is a common and highly effective approach to deep learning on small image datasets. It is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. Some pretrained models are VGG16, ResNet50, Inception V3, etc. Moreover, freezing means preventing weights from updating from training. Thus, we say we will freeze a layer or a set of layers. We do this because if we don’t, then there is no significance to using a pretrained model and what was learned before will now be modified. To do this we just set the network’s trainable attribute to False. \n",
        "\n",
        "Now we finally can understand what fine-tuning means. Fine-tuning is the method of unfreezing a few top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model and these top layers. We used this because it represents the process better. It slightly adjusts the model being reused in order to make it more relevant to the model we are trying to use. \n",
        "\n",
        "The steps for fine-tuning a network according to Rosebrock is:\n",
        "\n",
        "\n",
        "1.   Add your custom network on top of an already-trained base network.\n",
        "2.   Freeze the base network\n",
        "3. Train the part you added.\n",
        "4. Unfreeze some layers in the base network\n",
        "5. Jointly train both these layers and the part you added.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLK9Z8WDQnFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv2d_4':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}